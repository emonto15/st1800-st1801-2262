{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuración en google colab de spark y pyspark\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuración en google colab\n",
    "#instalar java y spark\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://downloads.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
    "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
    "!pip install findspark\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.-bin-hadoop3.2\"\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verificar que tengan instalado la librería 'pyspark'\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#forma 1 de crear la sesión y el contexto Spark:\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "#forma 2 de crear la sesión y el contexto Spark:\n",
    "#sc = SparkContext.getOrCreate()\n",
    "#spark=SparkSession.builder.appName('nlp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#myrdd = sc.wholeTextFiles('../datasets/papers_sample_pdf/*.txt')\n",
    "#df = myrdd.toDF(schema=['filename','content'])\n",
    "#df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame([(1,'I really liked this movie'),\n",
    "                         (2,'I would recommend this movie to my friends'),\n",
    "                         (3,'movie was alright but acting was horrible'),\n",
    "                         (4,'I am never watching that movie ever again')],\n",
    "                        ['user_id','content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenization=Tokenizer(inputCol='content',outputCol='tokens')\n",
    "tokenized_df=tokenization.transform(df)\n",
    "tokenized_df.printSchema()\n",
    "tokenized_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords removal \n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')\n",
    "refined_df=stopword_removal.transform(tokenized_df)\n",
    "refined_df.select(['tokens','refined_tokens']).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_udf = udf(lambda s: len(s), IntegerType())\n",
    "\n",
    "refined_count_df = refined_df.withColumn(\"token_count\", len_udf(col('refined_tokens')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_count_df.orderBy(rand()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "count_vec=CountVectorizer(inputCol='refined_tokens',outputCol='features')\n",
    "cv_df=count_vec.fit(refined_df).transform(refined_df)\n",
    "cv_df.select(['refined_tokens','features']).show(4,False)\n",
    "bow = count_vec.fit(refined_df).vocabulary\n",
    "print(bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF with HashingTF\n",
    "from pyspark.ml.feature import HashingTF\n",
    "# podria utilizar numFeatures como el tamaño del Bag of Words:\n",
    "l = len(bow)\n",
    "hashing_vec=HashingTF(inputCol='refined_tokens',outputCol='tf_features',numFeatures=l)\n",
    "#hashing_vec=HashingTF(inputCol='refined_tokens',outputCol='tf_features',numFeatures=11)\n",
    "# compare la salida e interprete con y sin numFeatures:\n",
    "#hashing_vec=HashingTF(inputCol='refined_tokens',outputCol='tf_features')\n",
    "\n",
    "hashing_df=hashing_vec.transform(refined_df)\n",
    "hashing_df.show(4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "tf_idf_vec=IDF(inputCol='tf_features',outputCol='tf_idf_features')\n",
    "tf_idf_df=tf_idf_vec.fit(hashing_df).transform(hashing_df)\n",
    "tf_idf_df.show(4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
